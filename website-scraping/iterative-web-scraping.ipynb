import requests
from bs4 import BeautifulSoup
import csv
import re
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin
import time

# Configuration
INPUT_CSV = "websites.csv"
OUTPUT_DIR = "scraped_policies"
MAX_RETRIES = 2
TIMEOUT = 10
MAX_THREADS = 5  # Avoid overwhelming servers

# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
}

def clean_text(text):
    """Remove excessive whitespace and special chars."""
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def scrape_policy(url, domain):
    """Scrape and clean privacy policy text."""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=headers, timeout=TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Remove unwanted elements
            for element in soup(['script', 'style', 'nav', 'footer', 'head', 'iframe', 'button']):
                element.decompose()
            
            # Try common content containers
            selectors = ['main', 'article', 'div.content', 'div.policy', 'div.datenschutz']
            main_content = None
            for selector in selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if not main_content:
                main_content = soup  # Fallback to entire body
            
            # Extract text from paragraphs, headings, lists
            text_elements = main_content.find_all(['p', 'h1', 'h2', 'h3', 'li', 'section'])
            text = '\n\n'.join([elem.get_text(strip=True) for elem in text_elements if elem.get_text(strip=True)])
            
            return clean_text(text)
        
        except requests.exceptions.RequestException as e:
            print(f"Attempt {attempt + 1} failed for {domain}: {e}")
            time.sleep(2)  # Backoff between retries
    return None

def process_row(row):
    """Handle scraping for a single row in CSV."""
    domain = row['domain']
    url = row['privacy_policy_url']
    print(f"Scraping: {domain}")
    
    policy_text = scrape_policy(url, domain)
    if policy_text:
        # Save to file
        output_path = os.path.join(OUTPUT_DIR, f"{domain.replace('.', '_')}_policy.txt")
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(policy_text)
        return (domain, "Success")
    return (domain, "Failed")

def main():
    # Read CSV
    with open(INPUT_CSV, mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        rows = list(reader)
    
    # Parallel scraping
    successes = 0
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(process_row, row) for row in rows]
        for future in as_completed(futures):
            domain, status = future.result()
            if status == "Success":
                successes += 1
            print(f"{domain}: {status}")
    
    print(f"\nDone. Successfully scraped {successes}/{len(rows)} policies.")

if __name__ == "__main__":
    main()
