{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00dd84b",
   "metadata": {},
   "source": [
    "# GDPR compliance text classification (Multinomial NB & logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197252d6-9f1b-4fbb-8671-69c0a0248c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f71a26-ee4f-4bdb-83d4-17d6deffc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gdpr articles from json file\n",
    "with open(\"gdpr_articles_baseline.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract all sections into a flat list\n",
    "records = []\n",
    "for article in data:\n",
    "    article_number = article.get(\"article_number\")\n",
    "    article_title = article.get(\"article_title\")\n",
    "    for section in article.get(\"sections\", []):\n",
    "        for sec_num, sec_text in section.items():\n",
    "            records.append({\n",
    "                \"article_number\": article_number,\n",
    "                \"article_title\": article_title,\n",
    "                \"section_number\": sec_num,\n",
    "                \"section_text\": sec_text\n",
    "            })\n",
    "\n",
    "# convert to dataframe and create label column\n",
    "df_gdpr = pd.DataFrame(records)\n",
    "df_gdpr[\"label\"] = \"Art. \" + df_gdpr[\"article_number\"].astype(str)\n",
    "\n",
    "# combine policy and gdpr texts into tf-idf vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(df[\"paragraph_text\"].tolist() + df_gdpr[\"section_text\"].tolist())\n",
    "X_policy = X_all[:len(df)]\n",
    "X_gdpr = X_all[len(df):]\n",
    "\n",
    "# calculate similarity and assign best matching gdpr label to each paragraph\n",
    "similarity = cosine_similarity(X_policy, X_gdpr)\n",
    "best_matches = similarity.argmax(axis=1)\n",
    "df[\"label\"] = [df_gdpr[\"label\"].iloc[i] for i in best_matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6c06b",
   "metadata": {},
   "source": [
    "## Load paragraph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e4e244-d146-4f2c-91c1-6728ff4afede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'policy_sentences.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download punkt tokenizer for sentence splitting\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# set the folder where the txt files are\n",
    "data_dir = Path(\"data\")\n",
    "output_data = []\n",
    "\n",
    "# go through each txt file and split it into sentences\n",
    "for txt_file in data_dir.glob(\"*.txt\"):\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        content = f.read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if len(sentence.strip()) >= 20:  # skip very short sentences\n",
    "                output_data.append({\n",
    "                    \"paragraph_id\": f\"{txt_file.stem}_{i}\",\n",
    "                    \"source\": txt_file.name,\n",
    "                    \"paragraph_text\": sentence.strip()\n",
    "                })\n",
    "\n",
    "# save the result as json file\n",
    "output_json_path = \"policy_sentences.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "output_json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6645db9",
   "metadata": {},
   "source": [
    "## Load GDPR articles and create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9dadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the gdpr json file and load the data\n",
    "with open('gdpr_articles_baseline.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# collect all gdpr sections in a flat list\n",
    "records = []\n",
    "for article in data:\n",
    "    article_number = article.get(\"article_number\")\n",
    "    article_title = article.get(\"article_title\")\n",
    "    for section in article.get(\"sections\", []):\n",
    "        for sec_num, sec_text in section.items():\n",
    "            records.append({\n",
    "                \"article_number\": article_number,\n",
    "                \"article_title\": article_title,\n",
    "                \"section_number\": sec_num,\n",
    "                \"section_text\": sec_text\n",
    "            })\n",
    "\n",
    "# convert the list to a dataframe and create a label column\n",
    "df_gdpr = pd.DataFrame(records)\n",
    "df_gdpr[\"label\"] = \"Art. \" + df_gdpr[\"article_number\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844e5b6",
   "metadata": {},
   "source": [
    "## Create training set (simulated labels for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cc31e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>source</th>\n",
       "      <th>paragraph_text</th>\n",
       "      <th>label</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aol_com_policy_0</td>\n",
       "      <td>aol_com_policy.txt</td>\n",
       "      <td>Welcome to the Yahoo Privacy PolicyLast update...</td>\n",
       "      <td>Art. 24</td>\n",
       "      <td>0.068724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aol_com_policy_1</td>\n",
       "      <td>aol_com_policy.txt</td>\n",
       "      <td>We serve our consumers, partners, advertisers ...</td>\n",
       "      <td>Art. 57</td>\n",
       "      <td>0.073494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aol_com_policy_2</td>\n",
       "      <td>aol_com_policy.txt</td>\n",
       "      <td>If you have an existing Yahoo or AOL account, ...</td>\n",
       "      <td>Art. 58</td>\n",
       "      <td>0.115544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aol_com_policy_3</td>\n",
       "      <td>aol_com_policy.txt</td>\n",
       "      <td>If you have not yet agreed to this Privacy Pol...</td>\n",
       "      <td>Art. 41</td>\n",
       "      <td>0.098341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aol_com_policy_4</td>\n",
       "      <td>aol_com_policy.txt</td>\n",
       "      <td>For Yahoo products or services that are access...</td>\n",
       "      <td>Art. 99</td>\n",
       "      <td>0.367017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paragraph_id              source  \\\n",
       "0  aol_com_policy_0  aol_com_policy.txt   \n",
       "1  aol_com_policy_1  aol_com_policy.txt   \n",
       "2  aol_com_policy_2  aol_com_policy.txt   \n",
       "3  aol_com_policy_3  aol_com_policy.txt   \n",
       "4  aol_com_policy_4  aol_com_policy.txt   \n",
       "\n",
       "                                      paragraph_text    label  \\\n",
       "0  Welcome to the Yahoo Privacy PolicyLast update...  Art. 24   \n",
       "1  We serve our consumers, partners, advertisers ...  Art. 57   \n",
       "2  If you have an existing Yahoo or AOL account, ...  Art. 58   \n",
       "3  If you have not yet agreed to this Privacy Pol...  Art. 41   \n",
       "4  For Yahoo products or services that are access...  Art. 99   \n",
       "\n",
       "   similarity_score  \n",
       "0          0.068724  \n",
       "1          0.073494  \n",
       "2          0.115544  \n",
       "3          0.098341  \n",
       "4          0.367017  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load sentence data from policy_sentences.json\n",
    "with open(\"policy_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentence_data = json.load(f)\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(sentence_data)\n",
    "\n",
    "# simulate label assignment using cosine similarity\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_all = vectorizer.fit_transform(df[\"paragraph_text\"].tolist() + df_gdpr[\"section_text\"].tolist())\n",
    "\n",
    "# split tf-idf matrix into policy and gdpr parts\n",
    "X_policy = X_all[:len(df)]\n",
    "X_gdpr = X_all[len(df):]\n",
    "\n",
    "# calculate cosine similarity between each policy paragraph and gdpr section\n",
    "similarity = cosine_similarity(X_policy, X_gdpr)\n",
    "best_matches = similarity.argmax(axis=1)\n",
    "\n",
    "# assign the most similar gdpr label to each paragraph\n",
    "df[\"label\"] = [df_gdpr[\"label\"].iloc[i] for i in best_matches]\n",
    "df[\"similarity_score\"] = similarity.max(axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0930de",
   "metadata": {},
   "source": [
    "## Train classifier (MultinomialNB + TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aac96fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Art. 1       1.00      0.18      0.31        88\n",
      "     Art. 10       0.00      0.00      0.00         2\n",
      "     Art. 11       0.96      0.15      0.27       156\n",
      "     Art. 12       0.94      0.33      0.49       257\n",
      "     Art. 13       0.31      0.96      0.46      2460\n",
      "     Art. 14       0.63      0.68      0.65      1260\n",
      "     Art. 15       0.44      0.89      0.59      1531\n",
      "     Art. 16       0.00      0.00      0.00         3\n",
      "     Art. 17       0.97      0.38      0.54       402\n",
      "     Art. 18       0.99      0.50      0.66       268\n",
      "     Art. 19       0.00      0.00      0.00         1\n",
      "      Art. 2       0.94      0.28      0.43       112\n",
      "     Art. 20       0.96      0.30      0.46       160\n",
      "     Art. 21       1.00      0.26      0.42       242\n",
      "     Art. 22       0.98      0.33      0.50       276\n",
      "     Art. 23       0.84      0.79      0.82       972\n",
      "     Art. 24       0.00      0.00      0.00        15\n",
      "     Art. 25       1.00      0.25      0.40       207\n",
      "     Art. 26       1.00      0.07      0.12        75\n",
      "     Art. 27       1.00      0.11      0.19        56\n",
      "     Art. 28       0.96      0.18      0.30       148\n",
      "     Art. 29       0.00      0.00      0.00        20\n",
      "      Art. 3       1.00      0.20      0.34       193\n",
      "     Art. 30       0.98      0.49      0.65       368\n",
      "     Art. 31       0.00      0.00      0.00        19\n",
      "     Art. 32       0.94      0.37      0.54       283\n",
      "     Art. 33       0.85      0.39      0.54       432\n",
      "     Art. 34       1.00      0.27      0.43       121\n",
      "     Art. 35       0.97      0.34      0.50       206\n",
      "     Art. 36       1.00      0.34      0.50       190\n",
      "     Art. 37       1.00      0.17      0.30        98\n",
      "     Art. 38       0.98      0.51      0.67       368\n",
      "     Art. 39       1.00      0.15      0.26       113\n",
      "      Art. 4       0.93      0.66      0.77       754\n",
      "     Art. 40       0.73      0.79      0.76      1255\n",
      "     Art. 41       0.00      0.00      0.00        48\n",
      "     Art. 42       1.00      0.14      0.24        79\n",
      "     Art. 43       0.00      0.00      0.00        17\n",
      "     Art. 44       0.00      0.00      0.00         5\n",
      "     Art. 45       1.00      0.10      0.18       132\n",
      "     Art. 46       1.00      0.20      0.33       100\n",
      "     Art. 47       0.84      0.78      0.81       917\n",
      "     Art. 48       0.00      0.00      0.00         6\n",
      "     Art. 49       0.93      0.54      0.68       504\n",
      "      Art. 5       0.99      0.40      0.57       376\n",
      "     Art. 50       1.00      0.20      0.34        74\n",
      "     Art. 51       0.00      0.00      0.00        38\n",
      "     Art. 52       0.00      0.00      0.00        31\n",
      "     Art. 53       1.00      0.15      0.27        91\n",
      "     Art. 54       1.00      0.25      0.40       139\n",
      "     Art. 55       0.00      0.00      0.00        33\n",
      "     Art. 56       0.00      0.00      0.00        27\n",
      "     Art. 57       0.95      0.57      0.71       600\n",
      "     Art. 58       0.89      0.71      0.79       980\n",
      "     Art. 59       0.00      0.00      0.00        21\n",
      "      Art. 6       0.75      0.74      0.75      1103\n",
      "     Art. 60       1.00      0.21      0.34        53\n",
      "     Art. 61       0.98      0.33      0.50       322\n",
      "     Art. 62       0.00      0.00      0.00        26\n",
      "     Art. 64       1.00      0.16      0.28       110\n",
      "     Art. 65       1.00      0.16      0.28        56\n",
      "     Art. 66       0.00      0.00      0.00         3\n",
      "     Art. 68       0.00      0.00      0.00        31\n",
      "     Art. 69       0.00      0.00      0.00         7\n",
      "      Art. 7       0.93      0.17      0.28       227\n",
      "     Art. 70       0.92      0.43      0.59       385\n",
      "     Art. 71       1.00      0.14      0.24        44\n",
      "     Art. 72       0.00      0.00      0.00        38\n",
      "     Art. 73       1.00      0.43      0.60        37\n",
      "     Art. 74       0.00      0.00      0.00        10\n",
      "     Art. 75       0.98      0.40      0.56       348\n",
      "     Art. 76       0.00      0.00      0.00         1\n",
      "     Art. 77       0.00      0.00      0.00        13\n",
      "     Art. 78       0.00      0.00      0.00        30\n",
      "     Art. 79       0.00      0.00      0.00        20\n",
      "      Art. 8       0.98      0.75      0.85       252\n",
      "     Art. 80       0.00      0.00      0.00         8\n",
      "     Art. 81       0.00      0.00      0.00        24\n",
      "     Art. 82       1.00      0.36      0.53        47\n",
      "     Art. 83       0.99      0.48      0.65       259\n",
      "     Art. 84       0.00      0.00      0.00         1\n",
      "     Art. 85       0.00      0.00      0.00         1\n",
      "     Art. 86       0.00      0.00      0.00        13\n",
      "     Art. 87       0.00      0.00      0.00        47\n",
      "     Art. 88       1.00      0.07      0.13        73\n",
      "     Art. 89       1.00      0.18      0.31       120\n",
      "      Art. 9       0.67      0.74      0.70       849\n",
      "     Art. 90       0.00      0.00      0.00         5\n",
      "     Art. 91       0.00      0.00      0.00        11\n",
      "     Art. 92       0.00      0.00      0.00         4\n",
      "     Art. 93       0.00      0.00      0.00        17\n",
      "     Art. 94       1.00      0.67      0.80        27\n",
      "     Art. 95       1.00      0.55      0.71        11\n",
      "     Art. 96       0.00      0.00      0.00         5\n",
      "     Art. 97       1.00      0.15      0.26       101\n",
      "     Art. 98       0.00      0.00      0.00         3\n",
      "     Art. 99       0.95      0.36      0.53        55\n",
      "\n",
      "    accuracy                           0.59     22126\n",
      "   macro avg       0.58      0.24      0.30     22126\n",
      "weighted avg       0.76      0.59      0.58     22126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenaw\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenaw\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenaw\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"paragraph_text\"], df[\"label\"],\n",
    "    test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# convert text to tf-idf features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# train a naive bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c009e4",
   "metadata": {},
   "source": [
    "## Prediction function for new paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571245ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph: Please see ourLegal basespage for more information.Data Processing and TransfersWhen you use or interact with any of our Services, you consent to the data processing, sharing, transferring and uses of your information as outlined in this Privacy Policy.\n",
      "Top GDPR predictions:\n",
      "  Art. 23: 0.44\n",
      "  Art. 13: 0.12\n",
      "  Art. 40: 0.11\n",
      "  Art. 15: 0.09\n",
      "  Art. 47: 0.04\n"
     ]
    }
   ],
   "source": [
    "# return top n gdpr predictions for a given text\n",
    "def top_n_predictions(text, n=3):\n",
    "    vec = vectorizer.transform([text])  # convert text to tf-idf vector\n",
    "    proba = model.predict_proba(vec)[0]  # get class probabilities\n",
    "    top_indices = proba.argsort()[::-1][:n]  # get indices of top n classes\n",
    "    return [(model.classes_[i], round(proba[i], 4)) for i in top_indices]\n",
    "\n",
    "# get top 5 predictions\n",
    "text = df.iloc[100][\"paragraph_text\"]\n",
    "\n",
    "top_preds = top_n_predictions(text, n=5)\n",
    "\n",
    "# print the paragraph and its top predictions\n",
    "print(\"Paragraph:\", text[:300])\n",
    "print(\"Top GDPR predictions:\")\n",
    "for label, score in top_preds:\n",
    "    print(f\"  {label}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c07591-55fd-4c91-9472-e7a36ce2e94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multinomialNB_vectorizer.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(model, \"multinomialNB_model.joblib\")\n",
    "joblib.dump(vectorizer, \"multinomialNB_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d985ae-cd1e-4150-80cf-29fcb31382be",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a02854-0a52-4ef2-b87b-4572f683fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Art. 1       0.97      0.80      0.88        88\n",
      "     Art. 10       0.00      0.00      0.00         2\n",
      "     Art. 11       0.93      0.89      0.91       156\n",
      "     Art. 12       0.95      0.90      0.93       257\n",
      "     Art. 13       0.85      0.95      0.90      2460\n",
      "     Art. 14       0.91      0.92      0.92      1260\n",
      "     Art. 15       0.87      0.94      0.91      1531\n",
      "     Art. 16       0.00      0.00      0.00         3\n",
      "     Art. 17       0.93      0.89      0.91       402\n",
      "     Art. 18       0.96      0.86      0.91       268\n",
      "     Art. 19       0.00      0.00      0.00         1\n",
      "      Art. 2       0.97      0.87      0.92       112\n",
      "     Art. 20       0.97      0.88      0.92       160\n",
      "     Art. 21       0.95      0.93      0.94       242\n",
      "     Art. 22       0.97      0.92      0.94       276\n",
      "     Art. 23       0.91      0.95      0.93       972\n",
      "     Art. 24       1.00      0.73      0.85        15\n",
      "     Art. 25       0.95      0.93      0.94       207\n",
      "     Art. 26       1.00      0.92      0.96        75\n",
      "     Art. 27       1.00      0.73      0.85        56\n",
      "     Art. 28       0.97      0.84      0.90       148\n",
      "     Art. 29       1.00      0.80      0.89        20\n",
      "      Art. 3       0.98      0.91      0.94       193\n",
      "     Art. 30       0.94      0.92      0.93       368\n",
      "     Art. 31       0.90      1.00      0.95        19\n",
      "     Art. 32       0.90      0.85      0.87       283\n",
      "     Art. 33       0.90      0.91      0.91       432\n",
      "     Art. 34       1.00      0.93      0.97       121\n",
      "     Art. 35       0.95      0.88      0.91       206\n",
      "     Art. 36       0.99      0.86      0.92       190\n",
      "     Art. 37       0.98      0.83      0.90        98\n",
      "     Art. 38       0.95      0.94      0.95       368\n",
      "     Art. 39       0.97      0.88      0.92       113\n",
      "      Art. 4       0.93      0.89      0.91       754\n",
      "     Art. 40       0.92      0.94      0.93      1255\n",
      "     Art. 41       1.00      0.77      0.87        48\n",
      "     Art. 42       1.00      0.85      0.92        79\n",
      "     Art. 43       1.00      0.94      0.97        17\n",
      "     Art. 44       1.00      0.80      0.89         5\n",
      "     Art. 45       1.00      0.86      0.92       132\n",
      "     Art. 46       0.93      0.88      0.90       100\n",
      "     Art. 47       0.91      0.95      0.93       917\n",
      "     Art. 48       1.00      0.83      0.91         6\n",
      "     Art. 49       0.92      0.92      0.92       504\n",
      "      Art. 5       0.93      0.91      0.92       376\n",
      "     Art. 50       0.98      0.80      0.88        74\n",
      "     Art. 51       1.00      0.79      0.88        38\n",
      "     Art. 52       1.00      0.90      0.95        31\n",
      "     Art. 53       1.00      0.87      0.93        91\n",
      "     Art. 54       1.00      0.91      0.95       139\n",
      "     Art. 55       1.00      0.79      0.88        33\n",
      "     Art. 56       1.00      0.89      0.94        27\n",
      "     Art. 57       0.91      0.92      0.92       600\n",
      "     Art. 58       0.88      0.96      0.92       980\n",
      "     Art. 59       1.00      0.67      0.80        21\n",
      "      Art. 6       0.91      0.90      0.91      1103\n",
      "     Art. 60       1.00      0.79      0.88        53\n",
      "     Art. 61       0.92      0.91      0.91       322\n",
      "     Art. 62       1.00      0.88      0.94        26\n",
      "     Art. 64       0.96      0.89      0.92       110\n",
      "     Art. 65       1.00      0.84      0.91        56\n",
      "     Art. 66       0.00      0.00      0.00         3\n",
      "     Art. 68       1.00      0.90      0.95        31\n",
      "     Art. 69       1.00      0.71      0.83         7\n",
      "      Art. 7       0.95      0.96      0.95       227\n",
      "     Art. 70       0.93      0.90      0.92       385\n",
      "     Art. 71       1.00      0.95      0.98        44\n",
      "     Art. 72       1.00      0.74      0.85        38\n",
      "     Art. 73       1.00      0.89      0.94        37\n",
      "     Art. 74       1.00      0.30      0.46        10\n",
      "     Art. 75       0.97      0.91      0.94       348\n",
      "     Art. 76       0.00      0.00      0.00         1\n",
      "     Art. 77       1.00      0.85      0.92        13\n",
      "     Art. 78       0.97      0.93      0.95        30\n",
      "     Art. 79       1.00      0.95      0.97        20\n",
      "      Art. 8       0.96      0.98      0.97       252\n",
      "     Art. 80       1.00      0.88      0.93         8\n",
      "     Art. 81       1.00      0.71      0.83        24\n",
      "     Art. 82       1.00      0.79      0.88        47\n",
      "     Art. 83       0.98      0.86      0.92       259\n",
      "     Art. 84       0.00      0.00      0.00         1\n",
      "     Art. 85       0.00      0.00      0.00         1\n",
      "     Art. 86       1.00      0.92      0.96        13\n",
      "     Art. 87       1.00      0.91      0.96        47\n",
      "     Art. 88       1.00      0.86      0.93        73\n",
      "     Art. 89       1.00      0.98      0.99       120\n",
      "      Art. 9       0.92      0.96      0.94       849\n",
      "     Art. 90       1.00      0.60      0.75         5\n",
      "     Art. 91       1.00      0.64      0.78        11\n",
      "     Art. 92       0.00      0.00      0.00         4\n",
      "     Art. 93       1.00      0.82      0.90        17\n",
      "     Art. 94       1.00      1.00      1.00        27\n",
      "     Art. 95       1.00      0.64      0.78        11\n",
      "     Art. 96       1.00      0.60      0.75         5\n",
      "     Art. 97       0.98      0.93      0.95       101\n",
      "     Art. 98       1.00      0.67      0.80         3\n",
      "     Art. 99       0.98      0.76      0.86        55\n",
      "\n",
      "    accuracy                           0.92     22126\n",
      "   macro avg       0.89      0.79      0.83     22126\n",
      "weighted avg       0.92      0.92      0.92     22126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenaw\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenaw\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\lenaw\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"paragraph_text\"], df[\"label\"], \n",
    "    test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# convert text to tf-idf vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ff14-555e-4347-b09a-dbee6cc8ca43",
   "metadata": {},
   "source": [
    "## Prediction Function for New Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ac9382-8de3-44aa-ae2c-1d14b8c63ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph: Please see ourLegal basespage for more information.Data Processing and TransfersWhen you use or interact with any of our Services, you consent to the data processing, sharing, transferring and uses of your information as outlined in this Privacy Policy.\n",
      "Top GDPR predictions:\n",
      "  Art. 23: 0.77\n",
      "  Art. 15: 0.04\n",
      "  Art. 13: 0.03\n",
      "  Art. 9: 0.02\n",
      "  Art. 7: 0.01\n"
     ]
    }
   ],
   "source": [
    "# return top n gdpr predictions for a given text\n",
    "def top_n_predictions(text, n=5):\n",
    "    vec = vectorizer.transform([text])  # convert text to tf-idf vector\n",
    "    proba = model.predict_proba(vec)[0]  # get prediction probabilities\n",
    "    top_indices = proba.argsort()[::-1][:n]  # get indices of top n scores\n",
    "    return [(model.classes_[i], round(proba[i], 4)) for i in top_indices]\n",
    "\n",
    "# get top 5 predictions\n",
    "text = df.iloc[100][\"paragraph_text\"]\n",
    "top_preds = top_n_predictions(text, n=5)\n",
    "\n",
    "# print the paragraph and its top predictions\n",
    "print(\"Paragraph:\", text[:300])\n",
    "print(\"Top GDPR predictions:\")\n",
    "for label, score in top_preds:\n",
    "    print(f\"  {label}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e3670eb-3b95-46a5-970a-b3a2900bac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regression_vectorizer.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(model, \"logistic_regression_model.joblib\")\n",
    "joblib.dump(vectorizer, \"logistic_regression_vectorizer.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
